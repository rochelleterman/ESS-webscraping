---
title: "Lab 3 Solutions"
author: "3I: Webscraping and Data Management in R"
date: "Aug 2020"
output: html_document
---

In today's lecture, we introduced some tools to collect pieces of data from individual presidential documents. For this lab, we will be looking at __all__ documents in the database that contain the string "space exploration." Our goals in this problem set are:

1. To scrape all documents returned from [this search query](https://www.presidency.ucsb.edu/advanced-search?field-keywords=%22space+exploration%22&field-keywords2=&field-keywords3=&from%5Bdate%5D=&to%5Bdate%5D=&person2=&items_per_page=100)

2. To organize this data into a dataframe and ultimately output a CSV file.

Below, I've given you the code for a function that passes the URL of an individual document, scrapes the information from that document, and returns this information in a list.

But this is all I will be providing you. You must complete the rest of the task yourself.

Specifically, you should:

1. Write code that scrapes all documents, organizes the information in a dataframe, and writes a csv file.

2. The end goal should be a dataset identical to the one I've provided for you in `data/space.csv`.

3. Split the code up into discrete steps, each with their own corresponding Rmarkdown chunk.

4. Document (i.e. describe) each step in clear but concise Rmarkdown prose.

5. The final chunk should:
  * print the structure (`str`) of the final data frame.
  * write the dataframe to a csv file. 

Onward!

```{r}
library(tidyverse)
library(rvest)
library(stringr)
library(purrr)
library(lubridate)

scrape_docs <- function(URL){
  doc <- read_html(URL)

  speaker <- html_nodes(doc, ".diet-title a") %>% 
    html_text()
  
  date <- html_nodes(doc, ".date-display-single") %>%
    html_text() %>%
    mdy()
  
  title <- html_nodes(doc, "h1") %>%
    html_text()
  
  text <- html_nodes(doc, "div.field-docs-content") %>%
    html_text()
  
  all_info <- list(speaker = speaker, date = date, title = title, text = text)
  
  print(str_c("scraping: ", title))
  return(all_info)
}
```

### Solution

There are likely many ways to achieve this task. Here's one solution:

#### Step 1: Write function `scrape_urls` to scrape URLs of individual search results.

The following function passes a page of search results, and returns a vector of URLs, each URL corresponding to an individual document.

```{r, warning=FALSE, message=FALSE}
scrape_urls <- function(path) {
  
  #Download HTML of webpage
  html <- read_html(path) 
  
  #select element with document URLs
  links <- html_nodes(html, ".views-field-title a") %>% 
                html_attr("href")
  
  #output results
  return(links) 
}

scrape_test <- scrape_urls("https://www.presidency.ucsb.edu/advanced-search?field-keywords=%22space+exploration%22&field-keywords2=&field-keywords3=&from%5Bdate%5D=&to%5Bdate%5D=&person2=&items_per_page=100")

scrape_test[1:10]
```

#### Step 2. Iterate over results pager to collect all URLs

`scrape_urls` collects all of the relative URLs from the first page of our search results (100 documents). While this is a good start, we have 4 pages of search results (325 results total) and need to collect the URLs of ALL results, from ALL result pages.

First, let's grab the path of all 4 result pages, and store that result in an object called `all_pages`:

```{r}
all_pages <- str_c("https://www.presidency.ucsb.edu/advanced-search?field-keywords=%22space%20exploration%22&field-keywords2=&field-keywords3=&from%5Bdate%5D=&to%5Bdate%5D=&person2=&items_per_page=100&page=", 0:3)
```

Now, we can use `scrape_urls` to collect the URLs from all the pages of search results. We store the results as a character vector called `all_urls`. 

```{r, warning=FALSE, message=FALSE}
all_urls <- map(all_pages, scrape_urls) %>%
  unlist

# uncomment to test -- should be 325 docs
# length(all_urls)
```

#### Step 3. Modify to Full Path

The `HREF` we got above is what's called a *relative* URL: i.e., it looks like this:

`/documents/special-message-the-congress-relative-space-science-and-exploration`

as opposed to having a full path, like:

`http://www.presidency.ucsb.edu/documents/special-message-the-congress-relative-space-science-and-exploration`

The following code converts the relative paths to full paths, and saves them in an object called `all_full_urls`.

```{r, warning=FALSE, message=FALSE}
all_full_urls <- str_c("https://www.presidency.ucsb.edu", all_urls)
all_full_urls[1:10]
```

#### Step 4. Scrape Documents

Now that we have the full paths to each document, we're ready to scrape each document.

We'll use the `scrape_docs` function (given above), which  accepts a URL of an individual record, scrapes the page, and returns a list containing the document's date, speaker, title, and full text.

Using this function, we'll iterate over `all_full_urls` to collect information on all the documents. We save the result as a dataframe, with each row representing a document.

Note: This might take a few minutes.

```{r, warning=FALSE, message=FALSE, results='hide'}
final_df <- map_dfr(all_full_urls, scrape_docs)

# same as: map(all_full_urls, scrape_docs) %>% bind_rows()
```

#### Step 5. Print and write

We'll print the dataframe's structure, write the csv, and we're done!

```{r}
head(final_df)
write.csv(final_df, "data/space.csv", row.names = F)
```

#### FULL SCRIPT

```{r}
library(tidyverse)
library(rvest)
library(stringr)
library(purrr)
library(lubridate)

# function to scrape 1 doc
scrape_docs <- function(URL){
  doc <- read_html(URL)

  speaker <- html_nodes(doc, ".diet-title a") %>% 
    html_text()
  
  date <- html_nodes(doc, ".date-display-single") %>%
    html_text() %>%
    mdy()
  
  title <- html_nodes(doc, "h1") %>%
    html_text()
  
  text <- html_nodes(doc, "div.field-docs-content") %>%
    html_text()
  
  all_info <- list(speaker = speaker, date = date, title = title, text = text)
  
  print(str_c("scraping: ", title))
  return(all_info)
}

# function to scrape urls of individual documents from results page
scrape_urls <- function(path) {
  
  #Download HTML of webpage
  html <- read_html(path) 
  
  #select element with document URLs
  links <- html_nodes(html, ".views-field-title a") %>% 
                html_attr("href")
  
  #output results
  return(links) 
}

# store URLS of 4 result pages.
all_pages <- str_c("https://www.presidency.ucsb.edu/advanced-search?field-keywords=%22space%20exploration%22&field-keywords2=&field-keywords3=&from%5Bdate%5D=&to%5Bdate%5D=&person2=&items_per_page=100&page=", 0:3)

# iterate over all results pages and get links to ind. docs
all_urls <- map(all_pages, scrape_urls) %>% unlist

# convert relative to full urls
all_full_urls <- str_c("https://www.presidency.ucsb.edu", all_urls)

# scrape all 325 docs
final_df <- map_dfr(all_full_urls, scrape_docs) 

# write to csv
write.csv(final_df, "data/space.csv")

```
